<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		    <h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
    <div style="text-align: center;">Name: Mark MacDermott</div>
    <br>
    <div style="text-align: center;">
        Link to webpage: <a href="https://ranototaudio.github.io/184-hw/">HW Writeups</a><br>
        Link to GitHub repository: <a href="https://github.com/RanototAudio">ranototaudio</a>
    </div>

    <figure>
        <img src="def.jpg" alt="rano" style="width:50%"/>
    </figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>

		<p> 
		As part of tasks 1.1 and 1.2 I implemented a process for generating rays which are sent from the camera through points located within a single pixel.
		The first function I implemented was generate_ray(double x, double y) which takes in a normalized (x,y) ∈ [0,1] x [0,1] coordinate and outputs a ray in world space.
		Some brief definitions to serve as reminders:
		</p>

		<ul>
			<li>Image Space: 2D discrete coordinate system corresponding to pixel indices on the rendered image (e.g., (x,y) in pixel units).</li>
			<li>Camera Space: 3D coordinate system with the camera at the origin, camera axes aligned with its orientation. Used for projecting world-space geometry into a view-dependent frame.</li>
			<li>World Space: 3D global coordinate system defining absolute positions and orientations of all objects and the camera. Used as the primary reference for scene construction and object placement.</li>
			<li>Screen Space – A 2D coordinate system representing positions on the camera’s projection plane (sensor or virtual film) in the camera’s local coordinate frame. It is derived by projecting 3D points onto a canonical plane (usually 1 unit away from the pinhole along the camera’s -z axis). Unlike image space, which uses normalized pixel indices, screen space uses physical or frustum-relative coordinates [−tan⁡(hFov/2),+tan⁡(hFov/2)] directly tied to the camera’s field of view.</li>
		</ul> 

		<p>
		We first transform the image coordinates to screen space which exists within camera space, generate the ray in the camera space, and finally transform it into a ray in the world space.
		So to start we calculate the bounds of screen space and then use linear interpolation to calculate our new position.
		The horizontal/vertical field of view (hFov, vFov) define how wide/tall the camera’s frustum is.
		We convert the given hFov and vFov from degrees to radians for tan calculation.
		Finally the Z-coordinate for our final point is set at -1.0 because the screen exists as a plane.
		Thus (x, y) ∈ [0,1] is remapped to physical sensor coordinates ∈ [xmin, xmax] × [ymin, ymax]:
		</p>

		<pre><code>
		// map to screen
		double hFov_rad = hFov * PI / 180.0;
		double vFov_rad = vFov * PI / 180.0;

		double xmin = -tan(0.5 * hFov_rad);
		double xmax = tan(0.5 * hFov_rad);
		double ymin = -tan(0.5 * vFov_rad);
		double ymax = tan(0.5 * vFov_rad);

		double X_cam = xmin + (xmax - xmin) * x;
		double Y_cam = ymin + (ymax - ymin) * y;
		Vector3D P_cam(X_cam, Y_cam, -1.0);
		</code></pre>

		<p>
		Now that we have our point in screen space we are able to compute the ray in camera space simply by taking the unit vector of P_cam.
		We finally map the camera space view vector to world space using the provided c2w matrix. 
		The c2w columns are the camera’s basis vectors in world space (u, v, w)
		w is the forward view direction and points to the middle of screen space.
		u is the camera's right vector perpendicular to both w and v.
		v is the up vector relative to the camera.
		</p>

		<pre><code>
		//compute view direction
		Vector3D v_cam = P_cam.unit();

		//map to worldspace
		Vector3D v_world = (P_cam.x * c2w[0]) + (P_cam.y * c2w[1]) + (P_cam.z * c2w[2]);
		</code></pre>

		<p>
		Finally we return the Ray object with correct parameters. 
		nclip and fclip are both greater than zero and represent the so-called near and far clipping planes. 
		We consider everything that lies outside these two clipping planes invisible to the camera.
		this-> = o and v_world = d.
		</p>

		<pre><code>
		//construct ray
		Ray newRay = Ray(this->pos, v_world);
		newRay.min_t = this->nClip;
		newRay.max_t = this->fClip;

		return newRay;
		</code></pre>

		<p>
		The second function I implemented was raytrace_pixel() which estimates the radiance (color) for a single pixel (x, y) by Monte Carlo path tracing.
		It does this through the following steps. First generating multiple rays through that pixel at randomized positions.
		Second tracing each ray into the scene to estimate incoming light via global illumination.
		Third averaging the contributions of all samples to get the pixel’s final color.
		</p>

		<pre><code>
		int num_samples = ns_aa;
		Vector3D sum(0.0, 0.0, 0.0);
		
		for (int i = 0; i < num_samples; i++) {
			double x_norm = (x + gridSampler->get_sample().x) / sampleBuffer.w;
			double y_norm = (y + gridSampler->get_sample().y) / sampleBuffer.h;

			sum += est_radiance_global_illumination(camera->generate_ray(x_norm, y_norm));

		}
		sum = sum / num_samples;
		sampleBuffer.update_pixel(sum, x, y);
		</code></pre>

		
		<p><strong>Rendering Equation:</strong></p>
		<p>
		\[
		L_o(p, \omega_o) = L_e(p, \omega_o) +
		\int_{\Omega} f_r(p, \omega_i, \omega_o)\, L_i(p, \omega_i)\, \cos\theta_i \, d\omega_i
		\]
		This is implemented via est_radiance_global_illumination()
		</p>

		<p><strong>Monte Carlo Estimator:</strong></p>
		<p>
		\[
		\hat{L}_o(p, \omega_o) =
		\frac{1}{N} \sum_{i=1}^N
		\frac{f_r(p, \omega_i, \omega_o)\, L_i(p, \omega_i)\, \cos\theta_i}{p(\omega_i)}
		\]
		This is implemented via summing and averaging the radiance values returned.
		</p>

		<p>
		Next I implemented Triangle::has_intersection(...) and Triangle::intersect(...)
		The ray equation is defined as follows: r(t) = o + td. r(t) is a point along the ray at timestop t.
		o is the origin or start position of the ray at timestop zero. d is a vector representing the direction that the ray travels from the origin.
		In order to solve for t, the time at which the ray intersects the triangle, we will use the Möller–Trumbore Algorithm.
		The Möller–Trumbore algorithm solves the ray-triangle intersection analytically by expressing the intersection point in terms of barycentric coordinates.
		</p>

		<p>
		Given a triangle defined by vertices \(P_0\), \(P_1\), and \(P_2\), we can express any point
		within the triangle using <strong>barycentric coordinates</strong>:
		</p>

		<p>
		\[
		P = \alpha P_0 + \beta P_1 + \gamma P_2
		\]
		</p>

		<p>
		We define:
		\[
		b_1 = \beta, \quad b_2 = \gamma, \quad \alpha = 1 - b_1 - b_2
		\]
		</p>

		<p>
		Substituting \(\alpha\):
		\[
		P = (1 - b_1 - b_2)P_0 + b_1 P_1 + b_2 P_2
		\]
		</p>

		<p>
		The ray equation is:
		\[
		r(t) = O + tD
		\]
		Substitute \(P = r(t)\):
		\[
		O + tD = (1 - b_1 - b_2)P_0 + b_1 P_1 + b_2 P_2
		\]
		</p>

		<p>
		Rearrange:
		\[
		O - P_0 = -tD + b_1 (P_1 - P_0) + b_2 (P_2 - P_0)
		\]
		</p>

		<p>
		Expressed in matrix form:
		</p>

		<p>
		\[
		\begin{bmatrix}
		- D & P_1 - P_0 & P_2 - P_0
		\end{bmatrix}
		\begin{bmatrix}
		t \\[4pt]
		b_1 \\[4pt]
		b_2
		\end{bmatrix}
		=
		O - P_0
		\]
		</p>

		<p>
		For a system \(M x = b\), Cramer’s rule gives:
		</p>

		<p>
		\[
		x =
		\frac{1}{|M|}
		\begin{bmatrix}
		|M_1| \\[4pt]
		|M_2| \\[4pt]
		|M_3|
		\end{bmatrix}
		\]
		</p>

		<p>
		where \(M_i\) is matrix \(M\) with its \(i\)-th column replaced by \(b\).
		</p>

		<p>
		Define the helper vectors:
		</p>

		<p>
		\[
		\begin{aligned}
		S &= O - P_0 \\[4pt]
		S_1 &= D \times E_2 \\[4pt]
		S_2 &= S \times E_1
		\end{aligned}
		\]
		</p>

		<p>
		The determinant of \(M\) is:
		\[
		|M| = S_1 \cdot E_1
		\]
		</p>

		<p>
		Substitute into Cramer’s rule to directly solve for \(t, b_1, b_2\):
		</p>

		<p>
		\[
		\begin{bmatrix}
		t \\[4pt]
		b_1 \\[4pt]
		b_2
		\end{bmatrix}
		=
		\frac{1}{S_1 \cdot E_1}
		\begin{bmatrix}
		S_2 \cdot E_2 \\[4pt]
		S_1 \cdot S \\[4pt]
		S_2 \cdot D
		\end{bmatrix}
		\]
		</p>


		<p>
		A valid intersection occurs if:
		</p>

		<p>
		\[
		\begin{aligned}
		& b_1 \ge 0, \\
		& b_2 \ge 0, \\
		& b_1 + b_2 \le 1, \\
		& t \ge 0
		\end{aligned}
		\]
		</p>

		<p>
		This code is simply an implemention of the process.
		</p>

		<pre><code>
			Vector3D E1 = p2 - p1;
			Vector3D E2 = p3 - p1;
			Vector3D S = r.o - p1;
			Vector3D S1 = cross(r.d, E2);
			Vector3D S2 = cross(S, E1);

			Vector3D sols(dot(S2, E2), dot(S1, S), dot(S2, r.d));
			sols = (1.0 / dot(S1, E1)) * sols;

			double t = sols[0];
			double b1 = sols[1];
			double b2 = sols[2];

			// t within range and barycentric coordinates valid
			if (t > r.min_t && t < r.max_t && b1 >= 0 && b2 >= 0 && (b1 + b2) <= 1) {
			r.max_t = t;
			isect->t = t;
			isect->bsdf = get_bsdf();
			isect->primitive = this;
			isect->n = ((1 - b1 - b2) * n1 + b1 * n2 + b2 * n3).unit();
			return true;
			}
			return false;
			}
		</code></pre>

		<p>
		Finally I implemented the ray-sphere intersection: <code>Sphere::has_intersection(...)</code> and <code>Sphere::intersect(...)</code>.  
		For any point \( \mathbf{p} \) on a sphere's surface, the following equation is satisfied:
		</p>

		<p style="text-align: center;">
		\( (\mathbf{p} - \mathbf{c})^2 - R^2 = 0 \)
		</p>

		<p>
		where \( \mathbf{c} \) is the center of the sphere and \( R \) is its radius.  
		Substituting the ray equation \( \mathbf{r}(t) = \mathbf{o} + t \mathbf{d} \) for \( \mathbf{p} \), we get:
		</p>

		<p style="text-align: center;">
		\( (\mathbf{o} + t\mathbf{d} - \mathbf{c})^2 - R^2 = 0 \)
		</p>

		<p>
		Expanding and simplifying gives a quadratic in \( t \):
		</p>

		<p style="text-align: center;">
		\( at^2 + bt + c = 0 \)
		</p>

		<p>
		with coefficients:
		</p>

		<ul>
		<li>\( a = \mathbf{d} \cdot \mathbf{d} \)</li>
		<li>\( b = 2(\mathbf{o} - \mathbf{c}) \cdot \mathbf{d} \)</li>
		<li>\( c = (\mathbf{o} - \mathbf{c}) \cdot (\mathbf{o} - \mathbf{c}) - R^2 \)</li>
		</ul>

		<p>
		Finally, solve for \( t \) using the quadratic formula.
		</p>


		<div style="display: flex; gap: 20px; justify-content: center; align-items: flex-start;">
		<div style="text-align: center;">

		<img src="part1-banana.png" alt="Description 1" style="max-width: 100%; height: auto; width: 400px;">
		<p><strong>Figure 1:</strong> banana.dae</p>
		</div>
		<div style="text-align: center;">
		<img src="part1-CBempty-before-task-3.png" alt="Description 2" style="max-width: 100%; height: auto; width: 400px;">
		<p><strong>Figure 2:</strong> CBempty.dae</p>
		</div>
		</div>

		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 3: Direct Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 4: Global Illumination</h2>
		In this part we are tasked with implementing indirect lighting via Russian Roulette.
		As previously seen in part 3, direct lighting refers to light that travels from a light source to the surface directly without bouncing off any other surface.
		This was computed via the one_bounce_radiance() function.
		Indirect lighting is lighting that arrives at a point after bouncing one or more times off surfaces after originating from a light.
		<table border="1" cellpadding="8" cellspacing="0">
		<thead>
			<tr>
			<th>Type</th>
			<th>Bounce Count</th>
			<th>Source</th>
			<th>Visibility Required</th>
			</tr>
		</thead>
		<tbody>
			<tr>
			<td>Emission</td>
			<td>0</td>
			<td>Hit light directly</td>
			<td>Yes</td>
			</tr>
			<tr>
			<td>Direct lighting</td>
			<td>1</td>
			<td>From light</td>
			<td>Yes (not occluded)</td>
			</tr>
			<tr>
			<td>Indirect lighting</td>
			<td>≥2</td>
			<td>From scene</td>
			<td>Indirectly via paths</td>
			</tr>
		</tbody>
		</table>
		The function at_least_one_bounce_radiance(const Ray& r, const Intersection& isect) implements  recursive light integration for indirect lighting, augmenting it with direct lighting and Russian roulette for termination.
		The following steps are implemented:
		
<ul>
		<li>Construct a local coordinate frame at the intersection point using the surface normal</li>
		<li>Compute the outgoing direction \( w_{\text{out}} \) in local coordinates</li>
		<li>If <code>isAccumBounces</code> is true, always add direct lighting via <code>one_bounce_radiance()</code>; otherwise, add it only at the final bounce</li>
		<li>Check for base case: if <code>r.depth &le; 0</code>, terminate recursion and return <code>L_out</code></li>
		<li>Apply Russian roulette: terminate the path with probability \( 1 - p \), where <code>p = 0.65</code></li>
		<li>Sample a random incoming direction \( w_{\text{in}} \) and BSDF value \( f_r \) using <code>sample_f()</code>; get PDF of the sample</li>
		<li>Transform \( w_{\text{in}} \) to world space and spawn a new ray in that direction</li>
		<li>Intersect the new ray with the scene geometry</li>
		<li>If an intersection occurs:
			<ul>
			<li>Recursively compute indirect lighting at the new intersection point</li>
			<li>Weight the incoming light using the Monte Carlo estimator:<br>
				<code>fr * cos(θ) * L_indirect / (pdf * continuation)</code></li>
			<li>Accumulate the weighted result into <code>L_out</code></li>
			</ul>
		</li>
		<li>Return the total radiance <code>L_out</code></li>
		</ul>


		Depending on whether or not isAccumBounces is true, we either integrate light at every bounce or only at max_ray_depth.
		<pre><code>
		if (isAccumBounces)
			L_out += one_bounce_radiance(r, isect);
		else
		{
			if (max_ray_depth - r.depth + 1 == max_ray_depth)
				L_out += one_bounce_radiance(r, isect);
		}
		</code></pre>

		Next the sampling function sample_f() is used to implement importance sampling utilizing the BSDF at intersection point p.
		w_out is the outgoing direction of light traveling from point p to the camera, or in recursive calls traveling from p' to p.
		w_in is a sampled incoming light direction used to ray trace to the next bounce computed.
		pdf is the probability density function value for the sampled direction w_in, p(w_in).
		<pre><code>
		Vector3D w_in;
		double pdf;
		Vector3D bsdf_sample = isect.bsdf->sample_f(w_out, &w_in, &pdf);
		if (pdf == 0.0) return L_out;
		</code></pre>

		Next we can recursively compute the lighting equation.
		<p>
		<b>Indirect Lighting Estimator:</b><br>
		\[
			L_{\text{indirect}} = \frac{f_r \cdot \cos\theta \cdot L_{\text{child}}}{p \cdot q}
		\]
		</p>
		<pre><code>
		Intersection new_isect;
		if (bvh->intersect(new_ray, &new_isect))
		{
			Vector3D indirect_L = at_least_one_bounce_radiance(new_ray, new_isect);
			double cos_theta = w_in.z;
			if (cos_theta > 0 && pdf > 0)
				L_out += bsdf_sample * cos_theta * indirect_L / (pdf * continuation);
		}
		</code></pre>

		Russian Roulette  is a probabilistic ray termination method. Instead of letting all rays recursive until max depth,
		we instead decide whether to terminate the ray at each step based on a continuation probability.
		We must also divide L_out by the continuation probabiliy as indicated above to preserve unbiased behavior (as indicated above)
		<p>
		If you randomly decide to:
		</p>
		<ul>
		<li><strong>Terminate</strong> with probability \( 1 - p \): return 0</li>
		<li><strong>Continue</strong> with probability \( p \): return \( \frac{f(x)}{p} \)</li>
		</ul>
		<p>
		Then the expected value is:
		</p>
		<p>
		\[
		\mathbb{E}[X] = (1 - p) \cdot 0 + p \cdot \left( \frac{f(x)}{p} \right) = f(x)
		\]
		</p>

		<pre><code>
		// Stop recursion if we've reached depth 0
		if (r.depth <= 0) return L_out;

		// Russian Roulette termination (apply to all bounces, or after a minimum depth)
		const double continuation = 0.65;
		if (!coin_flip(continuation)) return L_out;
		</code></pre>



		<h2>Part 5: Adaptive Sampling</h2>
		Some pixels in a scene convergence faster or slower during monte carlo light transport integration. This is governed by the variance of the function being integrated.
		Variance of a sample point is determined by scene complexity. For example a point that lies on the boundry of a shadow will have high variance because
		a variety of rays with different radiance arrive at that point. Compare this to a fully lit area where most rays arriving at that point will have high radiance.

		<h3>Mathematical Reason</h3>
		<p>
		Let \( \hat{I}_n \) be the Monte Carlo estimator with \( n \) samples:
		</p>
		<p>
		\[
		\text{Var}[\hat{I}_n] = \frac{\sigma^2}{n}
		\]
		</p>
		<ul>
		<li><strong>Low</strong> \( \sigma^2 \): few samples needed to reduce error</li>
		<li><strong>High</strong> \( \sigma^2 \): many samples required</li>
		</ul>
		<p>
		Pixels with lower \( \sigma^2 \) (i.e., less noise in sampled radiance) converge faster to the true value.
		</p>

		<p>
		Let \( x_k \) be the illuminance of the \( k \)-th sample. We track:
		</p>

		<ul>
		<li>\( s_1 = \sum_{k=1}^{n} x_k \) &nbsp; (sum of samples)</li>
		<li>\( s_2 = \sum_{k=1}^{n} x_k^2 \) &nbsp; (sum of squares)</li>
		</ul>

		<p>
		From this we compute:
		</p>

		<p><strong>Mean:</strong></p>
		<p>
		\[
		\mu = \frac{s_1}{n}
		\]

		<p><strong>Variance:</strong></p>
		<p>
		\[
		\sigma^2 = \frac{1}{n - 1} \left( s_2 - \frac{s_1^2}{n} \right)
		\]
		</p>

		<p>
		To measure convergence, we define a confidence interval on the sample mean using the sample variance and sample count:
		</p>
		<p>
		\[
		I = 1.96 \cdot \frac{\sigma}{\sqrt{n}}
		\]
		</p>
		<p>
		where:
		</p>
		<ul>
		<li>\( \sigma \): standard deviation of illuminance</li>
		<li>\( n \): number of samples traced so far</li>
		</ul>

		<p>
		With 95% confidence, the true mean illuminance lies within:
		</p>
		<p>
		\[
		\mu \pm I = \left[ \mu - 1.96 \cdot \frac{\sigma}{\sqrt{n}}, \; \mu + 1.96 \cdot \frac{\sigma}{\sqrt{n}} \right]
		\]
		</p>
		<p>
		This means there is a 95% probability that the actual average illuminance falls within this range given the observed variance.
		</p>

		<p>
		To determine convergence, we use the criterion:
		</p>
		<p>
		\[
		I \leq \text{maxTolerance} \cdot \mu
		\]
		</p>
		<p>
		If this holds true, we assume the pixel has sufficiently converged and terminate further sampling. 
		By default, <code>maxTolerance = 0.05</code>.
		</p>

		<p>
		Intuitively, \( I \) is small only when the samples’ variance \( \sigma^2 \) is small, 
		or the number of samples \( n \) is large enough. 
		So, the smaller \( I \) is, the more confidently we can conclude that the pixel has converged.
		</p>

		<pre><code>
		void PathTracer::raytrace_pixel(size_t x, size_t y) {

		int num_samples = ns_aa;
		Vector3D sum(0.0, 0.0, 0.0);

		int n = 0;
		double s1 = 0;
		double s2 = 0;
		
		while (n < num_samples) {
			double x_norm = (x + gridSampler->get_sample().x) / sampleBuffer.w;
			double y_norm = (y + gridSampler->get_sample().y) / sampleBuffer.h;
			Ray ray = camera->generate_ray(x_norm, y_norm);
			ray.depth = max_ray_depth;
			Vector3D radiance = est_radiance_global_illumination(ray);
			sum += radiance;

			n += 1;
			double illuminance = radiance.illum();
			s1 += illuminance;
			s2 += illuminance * illuminance;

			if (n % samplesPerBatch == 0 && n > 1) {
				double mean = s1 / n;
				double variance = (s2 - ((s1 * s1) / n)) / (n - 1);
				double I = 1.96 * sqrt(variance / n);
				if (I <= maxTolerance * mean) {
					break;
				}
			}
		}
		
		sum = sum / n;
		sampleBuffer.update_pixel(sum, x, y);
		sampleCountBuffer[x + y * sampleBuffer.w] = n;
		}
		</code></pre>

		</div>
	</body>
</html>